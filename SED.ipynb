{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoang\\anaconda3\\envs\\bird\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\hoang\\anaconda3\\envs\\bird\\lib\\site-packages\\cupyx\\jit\\_interface.py:173: FutureWarning: cupyx.jit.rawkernel is experimental. The interface can change in the future.\n",
      "  cupy._util.experimental('cupyx.jit.rawkernel')\n"
     ]
    }
   ],
   "source": [
    "import os, random\n",
    "import cv2\n",
    "import math\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from torchvision.models import efficientnet\n",
    "from torchvision.transforms import transforms\n",
    "# from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "import timm\n",
    "\n",
    "import scikitplot as skplt\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, KFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from glob import glob\n",
    "from IPython.display import display, Audio\n",
    "\n",
    "import cupy as cp\n",
    "from cupyx.scipy import signal as cupy_signal\n",
    "import yaml\n",
    "\n",
    "from metric import score\n",
    "\n",
    "import wandb\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(tar)\n\u001b[0;32m      8\u001b[0m target_smooth \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclamp(target\u001b[39m.\u001b[39mfloat(), \u001b[39m0.0025\u001b[39m, \u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m \u001b[39m0.0025\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m target_smooth \u001b[39m=\u001b[39m target_smooth \u001b[39m+\u001b[39m (\u001b[39m0.0025\u001b[39m \u001b[39m/\u001b[39m target\u001b[39m.\u001b[39;49msize(\u001b[39m1\u001b[39;49m))\n\u001b[0;32m     10\u001b[0m output \u001b[39m=\u001b[39m loss(\u001b[39minput\u001b[39m, target)\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39minput\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "inn = np.array([0,1,1,1], dtype=np.float32)\n",
    "input = torch.from_numpy(inn)\n",
    "\n",
    "tar = np.array([0,1,0,0], dtype=np.float32)\n",
    "target = torch.from_numpy(tar)\n",
    "target_smooth = torch.clamp(target.float(), 0.0025, 1.0 - 0.0025)\n",
    "target_smooth = target_smooth + (0.0025 / target.size(1))\n",
    "output = loss(input, target)\n",
    "print(input)\n",
    "print(target)\n",
    "print(target_smooth)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oog2spec_via_cupy(audio_data):\n",
    "    \n",
    "    audio_data = cp.array(audio_data)\n",
    "    \n",
    "    # handles NaNs\n",
    "    mean_signal = cp.nanmean(audio_data)\n",
    "    audio_data = cp.nan_to_num(audio_data, nan=mean_signal) if cp.isnan(audio_data).mean() < 1 else cp.zeros_like(audio_data)\n",
    "    \n",
    "    # to spec.\n",
    "    frequencies, times, spec_data = cupy_signal.spectrogram(\n",
    "        audio_data, \n",
    "        fs=48000, \n",
    "        nfft=1095, \n",
    "        nperseg=412, \n",
    "        noverlap=100, \n",
    "        window='hann'\n",
    "    )\n",
    "    \n",
    "    # Filter frequency range\n",
    "    valid_freq = (frequencies >= 40) & (frequencies <= 15000)\n",
    "    spec_data = spec_data[valid_freq, :]\n",
    "    \n",
    "    # Log\n",
    "    spec_data = cp.log10(spec_data + 1e-20)\n",
    "    \n",
    "    # min/max normalize\n",
    "    spec_data = spec_data - spec_data.min()\n",
    "    spec_data = spec_data / spec_data.max()\n",
    "    \n",
    "    return spec_data.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data, _ = librosa.load(\"inputs/previous_dataset/ColumbiaAndCostaRica/soundscape_data/NES_001_S01_20190914_043000.flac\", sr=48000)\n",
    "\n",
    "# ogg to spec.\n",
    "input_spec = oog2spec_via_cupy(audio_data)\n",
    "\n",
    "# input_spec = cv2.resize(input_spec, (256, 256), interpolation=cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'VERSION': 'v1.3',\n",
       " 'DESCRIPTION': 'Get all 5s data',\n",
       " 'DATA_PATH': 'inputs',\n",
       " 'LOAD_SPEC_DATA': True,\n",
       " 'SEED': 24,\n",
       " 'SAMPLE_RATE': 32000,\n",
       " 'N_FFT': 1095,\n",
       " 'WIN_SIZE': 412,\n",
       " 'WIN_LAP': 100,\n",
       " 'MIN_FREQ': 40,\n",
       " 'MAX_FREQ': 15000,\n",
       " 'EPOCHS': 10,\n",
       " 'FOLD': 5,\n",
       " 'BACTHSIZE': 16,\n",
       " 'LABEL_SMOOTHING': 0.0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_config = {\n",
    "    \"VERSION\": \"v0.3\",\n",
    "    \"DATA_PATH\": \"inputs\",\n",
    "    \"LOAD_SPEC_DATA\": True,\n",
    "    \"SEED\": 24,\n",
    "    \"SAMPLE_RATE\": 32000,\n",
    "    \"N_FFT\": 1095,\n",
    "    \"WIN_SIZE\": 412,\n",
    "    \"WIN_LAP\": 100,\n",
    "    \"MIN_FREQ\": 40,\n",
    "    \"MAX_FREQ\": 15000,\n",
    "    \"EPOCHS\": 10,\n",
    "    \"BACHSIZE\": 16\n",
    "}\n",
    "\n",
    "try:\n",
    "    with open('config.yaml', 'r') as f:\n",
    "        default_config = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "default_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdCLEF_Model_EfficientnetB0(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super(BirdCLEF_Model_EfficientnetB0, self).__init__()\n",
    "        self.backbone = timm.create_model('tf_efficientnet_b0.in1k', pretrained=True, in_chans=1,  num_classes=num_class)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randn(default_config[\"BACTHSIZE\"], 1, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "import openvino as ov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model/v1.3/BaseModel_EfficientB0_Fold3.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m fold \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(default_config[\u001b[39m\"\u001b[39m\u001b[39mFOLD\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[0;32m      2\u001b[0m     bird_model \u001b[39m=\u001b[39m BirdCLEF_Model_EfficientnetB0(num_class\u001b[39m=\u001b[39m\u001b[39m182\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmodel/\u001b[39;49m\u001b[39m{\u001b[39;49;00mdefault_config[\u001b[39m'\u001b[39;49m\u001b[39mVERSION\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m}\u001b[39;49;00m\u001b[39m/BaseModel_EfficientB0_Fold\u001b[39;49m\u001b[39m{\u001b[39;49;00mfold\u001b[39m}\u001b[39;49;00m\u001b[39m.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m, map_location\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m      4\u001b[0m     bird_model\u001b[39m.\u001b[39mload_state_dict(weights)\n\u001b[0;32m      5\u001b[0m     bird_model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\hoang\\anaconda3\\envs\\bird\\lib\\site-packages\\torch\\serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    996\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 998\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    999\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1000\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\hoang\\anaconda3\\envs\\bird\\lib\\site-packages\\torch\\serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    444\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 445\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    446\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\hoang\\anaconda3\\envs\\bird\\lib\\site-packages\\torch\\serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 426\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model/v1.3/BaseModel_EfficientB0_Fold3.pt'"
     ]
    }
   ],
   "source": [
    "for fold in range(default_config[\"FOLD\"]):\n",
    "    bird_model = BirdCLEF_Model_EfficientnetB0(num_class=182)\n",
    "    weights = torch.load(f\"model/{default_config['VERSION']}/BaseModel_EfficientB0_Fold{fold}.pt\", map_location=torch.device('cpu'))\n",
    "    bird_model.load_state_dict(weights)\n",
    "    bird_model.eval()\n",
    "\n",
    "    ov_model = ov.convert_model(bird_model, example_input=input_tensor)\n",
    "    ov.save_model(ov_model, f\"model/{default_config['VERSION']}/BaseModel_EfficientB0_Fold{fold}.xml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bird",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
